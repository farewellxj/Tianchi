{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn import cross_validation\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('jinnan_round1_train_20181227.csv',encoding='gbk')\n",
    "test = pd.read_csv('jinnan_round1_testA_20181227.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除类别唯一的特征\n",
    "for df in [train, test]:\n",
    "    df.drop(['B3', 'B13', 'A13', 'A18', 'A23'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 0.9863896848137536\n",
      "A2 0.9699140401146131\n",
      "A3 0.9570200573065902\n",
      "A4 0.9570200573065902\n",
      "B2 0.9842406876790831\n"
     ]
    }
   ],
   "source": [
    "# 删除某一类别占比超过90%的列\n",
    "good_cols = list(train.columns)\n",
    "for col in train.columns:\n",
    "    rate = train[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.9:\n",
    "        good_cols.remove(col)\n",
    "        print(col,rate)\n",
    "\n",
    "# 删除异常值\n",
    "train = train[train['收率']>0.87]\n",
    "        \n",
    "train = train[good_cols]\n",
    "good_cols.remove('收率')\n",
    "test  = test[good_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "target = train['收率']\n",
    "del train['收率']\n",
    "data = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "data = data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeTranSecond(t):\n",
    "    try:\n",
    "        t,m,s=t.split(\":\")\n",
    "    except:\n",
    "        if t=='1900/1/9 7:00':\n",
    "            return 7*3600/3600\n",
    "        elif t=='1900/1/1 2:30':\n",
    "            return (2*3600+30*60)/3600\n",
    "        elif t==-1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    try:\n",
    "        tm = (int(t)*3600+int(m)*60+int(s))/3600\n",
    "    except:\n",
    "        return (30*60)/3600\n",
    "    \n",
    "    return tm\n",
    "for f in ['A5','A7','A9','A11','A14','A16','A24','A26','B5','B7']:\n",
    "    try:\n",
    "        data[f] = data[f].apply(timeTranSecond)\n",
    "    except:\n",
    "        print(f,'应该在前面被删除了！')\n",
    "\n",
    "def getDuration(se):\n",
    "    try:\n",
    "        sh,sm,eh,em=re.findall(r\"\\d+\\.?\\d*\",se)\n",
    "    except:\n",
    "        if se == -1:\n",
    "            return -1 \n",
    "        \n",
    "    try:\n",
    "        if int(sh)>int(eh):\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600 + 24\n",
    "        else:\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600\n",
    "    except:\n",
    "        if se=='19:-20:05':\n",
    "            return 1\n",
    "        elif se=='15:00-1600':\n",
    "            return 1\n",
    "    \n",
    "    return tm\n",
    "for f in ['A20','A28','B4','B9','B10','B11']:\n",
    "    data[f] = data.apply(lambda df: getDuration(df[f]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['样本id'] = data['样本id'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "categorical_columns = [f for f in data.columns if f not in ['样本id']]\n",
    "numerical_columns = [f for f in data.columns if f not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 33)\n",
      "(150, 33)\n"
     ]
    }
   ],
   "source": [
    "#label encoder\n",
    "for f in categorical_columns:\n",
    "    data[f] = data[f].map(dict(zip(data[f].unique(), range(0, data[f].nunique()))))\n",
    "train = data[:train.shape[0]]\n",
    "test  = data[train.shape[0]:]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 108)\n",
      "(150, 108)\n"
     ]
    }
   ],
   "source": [
    "#train['target'] = list(target) \n",
    "train['target'] = target\n",
    "train['intTarget'] = pd.cut(train['target'], 5, labels=False)\n",
    "train = pd.get_dummies(train, columns=['intTarget'])\n",
    "li = ['intTarget_0.0','intTarget_1.0','intTarget_2.0','intTarget_3.0','intTarget_4.0']\n",
    "mean_columns = []\n",
    "for f1 in categorical_columns:\n",
    "    cate_rate = train[f1].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if cate_rate < 0.90:\n",
    "        for f2 in li:\n",
    "            col_name = 'B14_to_'+f1+\"_\"+f2+'_mean'\n",
    "            mean_columns.append(col_name)\n",
    "            order_label = train.groupby([f1])[f2].mean()\n",
    "            train[col_name] = train['B14'].map(order_label)\n",
    "            miss_rate = train[col_name].isnull().sum() * 100 / train[col_name].shape[0]\n",
    "            if miss_rate > 0:\n",
    "                train = train.drop([col_name], axis=1)\n",
    "                mean_columns.remove(col_name)\n",
    "            else:\n",
    "                test[col_name] = test['B14'].map(order_label)\n",
    "                \n",
    "train.drop(li+['target'], axis=1, inplace=True)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 1245)\n",
      "(150, 1245)\n"
     ]
    }
   ],
   "source": [
    "X_train = train[mean_columns+numerical_columns].values\n",
    "X_test = test[mean_columns+numerical_columns].values\n",
    "# one hot\n",
    "enc = OneHotEncoder()\n",
    "for f in categorical_columns:\n",
    "    enc.fit(data[f].values.reshape(-1, 1))\n",
    "    X_train = sparse.hstack((X_train, enc.transform(train[f].values.reshape(-1, 1))), 'csr')\n",
    "    X_test = sparse.hstack((X_test, enc.transform(test[f].values.reshape(-1, 1))), 'csr')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000229624\tvalid_1's l2: 0.000273472\n",
      "[400]\ttraining's l2: 0.00015443\tvalid_1's l2: 0.000198242\n",
      "[600]\ttraining's l2: 0.000126734\tvalid_1's l2: 0.000169483\n",
      "[800]\ttraining's l2: 0.000114587\tvalid_1's l2: 0.000157977\n",
      "[1000]\ttraining's l2: 0.000107763\tvalid_1's l2: 0.000152152\n",
      "[1200]\ttraining's l2: 0.00010316\tvalid_1's l2: 0.000148404\n",
      "[1400]\ttraining's l2: 9.99909e-05\tvalid_1's l2: 0.000145559\n",
      "[1600]\ttraining's l2: 9.7583e-05\tvalid_1's l2: 0.000143689\n",
      "[1800]\ttraining's l2: 9.54951e-05\tvalid_1's l2: 0.000142104\n",
      "[2000]\ttraining's l2: 9.37515e-05\tvalid_1's l2: 0.000140864\n",
      "[2200]\ttraining's l2: 9.23157e-05\tvalid_1's l2: 0.000139799\n",
      "[2400]\ttraining's l2: 9.15814e-05\tvalid_1's l2: 0.00013928\n",
      "Early stopping, best iteration is:\n",
      "[2312]\ttraining's l2: 9.15814e-05\tvalid_1's l2: 0.00013928\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000246916\tvalid_1's l2: 0.000265947\n",
      "[400]\ttraining's l2: 0.000167466\tvalid_1's l2: 0.00020452\n",
      "[600]\ttraining's l2: 0.000134208\tvalid_1's l2: 0.000175427\n",
      "[800]\ttraining's l2: 0.000119674\tvalid_1's l2: 0.000162531\n",
      "[1000]\ttraining's l2: 0.000111083\tvalid_1's l2: 0.000155321\n",
      "[1200]\ttraining's l2: 0.000105197\tvalid_1's l2: 0.000150528\n",
      "[1400]\ttraining's l2: 0.000101194\tvalid_1's l2: 0.000147706\n",
      "[1600]\ttraining's l2: 9.79145e-05\tvalid_1's l2: 0.000145303\n",
      "[1800]\ttraining's l2: 9.5569e-05\tvalid_1's l2: 0.000143731\n",
      "[2000]\ttraining's l2: 9.35182e-05\tvalid_1's l2: 0.000142537\n",
      "[2200]\ttraining's l2: 9.2031e-05\tvalid_1's l2: 0.000141759\n",
      "[2400]\ttraining's l2: 9.06702e-05\tvalid_1's l2: 0.000140988\n",
      "[2600]\ttraining's l2: 8.94188e-05\tvalid_1's l2: 0.000140298\n",
      "[2800]\ttraining's l2: 8.82848e-05\tvalid_1's l2: 0.000139737\n",
      "[3000]\ttraining's l2: 8.72939e-05\tvalid_1's l2: 0.000139191\n",
      "[3200]\ttraining's l2: 8.644e-05\tvalid_1's l2: 0.000138782\n",
      "Early stopping, best iteration is:\n",
      "[3245]\ttraining's l2: 8.62629e-05\tvalid_1's l2: 0.000138708\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000249663\tvalid_1's l2: 0.000258435\n",
      "[400]\ttraining's l2: 0.000170958\tvalid_1's l2: 0.000178102\n",
      "[600]\ttraining's l2: 0.000138396\tvalid_1's l2: 0.000147027\n",
      "[800]\ttraining's l2: 0.000124768\tvalid_1's l2: 0.000134641\n",
      "[1000]\ttraining's l2: 0.000117035\tvalid_1's l2: 0.000128564\n",
      "[1200]\ttraining's l2: 0.000111891\tvalid_1's l2: 0.000125042\n",
      "[1400]\ttraining's l2: 0.00010771\tvalid_1's l2: 0.00012227\n",
      "[1600]\ttraining's l2: 0.000104311\tvalid_1's l2: 0.000120063\n",
      "[1800]\ttraining's l2: 0.000101738\tvalid_1's l2: 0.00011855\n",
      "[2000]\ttraining's l2: 9.95272e-05\tvalid_1's l2: 0.000117582\n",
      "[2200]\ttraining's l2: 9.78044e-05\tvalid_1's l2: 0.000116751\n",
      "[2400]\ttraining's l2: 9.62322e-05\tvalid_1's l2: 0.000115877\n",
      "[2600]\ttraining's l2: 9.49224e-05\tvalid_1's l2: 0.0001152\n",
      "[2800]\ttraining's l2: 9.36842e-05\tvalid_1's l2: 0.000114747\n",
      "[3000]\ttraining's l2: 9.25713e-05\tvalid_1's l2: 0.000114419\n",
      "Early stopping, best iteration is:\n",
      "[3090]\ttraining's l2: 9.21719e-05\tvalid_1's l2: 0.000114354\n",
      "CV score: 0.00013079\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 125,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.008,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'mse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "folds = KFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 100)\n",
    "    oof_lgb[val_idx] = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(x_,y_,params_):\n",
    "    gbp = GridSearchCV(estimator = lgb.LGBMModel(boosting_type='gbdt',learning_rate=0.03,n_estimator=10,num_leaves=35,\n",
    "                                                  objective='regression',subsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0,\n",
    "                                                   random_state=None, n_jobs=-1, silent=True),\n",
    "                        param_grid = params_, scoring='neg_mean_squared_error',cv=5)\n",
    "    gbp.fit(x_,y_)\n",
    "    print(gbp.best_score_,gbp.grid_scores_,gbp.best_params_)\n",
    "    return gbp.best_params_\n",
    "\n",
    "\n",
    "def get_rmse(x_t,y_t,model_name):\n",
    "    y_p = model_name.predict(x_t)\n",
    "    rmse =  mean_squared_error(y_t, y_p)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00015122, -0.00012757, -0.00016375])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMModel(boosting_type='gbdt',learning_rate=0.04,n_estimator=10,num_leaves=35,max_depth = 20,\n",
    "                           objective='regression',subsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0,\n",
    "                           random_state=None, n_jobs=-1, silent=True).fit(X_train,y_train)\n",
    "\n",
    "cross_validation.cross_val_score(model_lgb, X_train,y_train, n_jobs=-1,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse =  0.00008140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.78751662e-05, 7.17664605e-05, 8.31716716e-05])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pre = clf.predict(X_train)\n",
    "lgb_pre = model_lgb.predict(X_train)\n",
    "stack = pd.DataFrame()\n",
    "stack['xgb'] = xgb_pre\n",
    "stack['lgb'] = lgb_pre\n",
    "\n",
    "rg = RidgeCV(cv = 5).fit(stack,y_train)\n",
    "print('mse =  %.8f' %mean_squared_error(y_train,rg.predict(stack)))\n",
    "-cross_validation.cross_val_score(rg,stack,y_train, n_jobs=-1,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = clf.predict(X_test)\n",
    "lgb_test = model_lgb.predict(X_test)\n",
    "\n",
    "stack_pre = pd.DataFrame()\n",
    "stack_pre['xgb'] = xgb_test\n",
    "stack_pre['lgb'] = lgb_test\n",
    "\n",
    "result = rg.predict(stack_pre)\n",
    "result = pd.DataFrame(result)\n",
    "result = result.apply(lambda x:round(x,3))\n",
    "result.to_csv('fff2.csv',header=None,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
